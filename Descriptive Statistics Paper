\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{setspace}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{url}
\setstretch{1.1}
\pagenumbering{gobble}

\begin{document}

\noindent \textbf{STAT 333 Final Project}  \\
\textbf{Student:} Duc Huy Nguyen (Thomas), Theo Schouten \\
\textbf{Date:} \today \\
\textbf{Subject:} Deliverable 2, Descriptive Statistical Analysis

\vspace{1em}

\noindent \textbf{Overview:} \\
This paper summarizes our preliminary findings of the dataset in order to serve two perposes:
\begin{itemize}
    \item Predicting whether a given Super Mario run will be the new world record (WR)
    \item Predicting when top speed runners will be able to reach the theoretical tool-assisted speedrun time.
\end{itemize}


\vspace{0.8em}

\noindent \textbf{Descriptive Statistics and Key Findings} \\
The datasets consists of two datasets:
\begin{itemize}
    \item The first dataset records the World Record (WR) progression from the first WR ever recorded in June 25th, 2002 to the latest WR, which occurred October 22nd, 2025. It can be retrieved here: \url {https://www.speedrun.com/smb1/stats?h=Any-NTSC&x=w20p0zkn-onvvdymn.013zwgxq}
    \begin{itemize}
        \item The first dataset contains the progression of the Super Mario Bros. Any\% (NTSC) World Record (WR), beginning with the earliest recorded WR on June 25, 2002 and continuing through the most recent WR on October 22, 2025. It can be accessed here: \url{https://www.speedrun.com/smb1/stats?h=Any-NTSC&x=w20p0zkn-onvvdymn.013zwgxq}
        \item The WR has been broken \textbf{46} times by \textbf{14} different players.
        \item The first recorded WR is \textbf{325 seconds} (5 minutes 25 seconds), set on June 25, 2002 by Aaron Collins. The current WR is \textbf{294.448 seconds} (4 minutes 54.448 seconds), set on October 22, 2025 by Niftski.
        \item The theoretical human-limit time, i.e., the fastest time a human could possibly achieve, is \textbf{294.032 seconds} (4 minutes 54.032 seconds).
        \item The current WR is \textbf{0.186 seconds} slower than the TAS (Tool-Assisted Speedrun) time, which corresponds to approximately \textbf{11 frames} at 60 frames per second.
        \item The median rate of WR improvement is \textbf{-0.00184 seconds per day}, while the most recent improvement rate is \textbf{-0.00243 seconds per day}.
        \item The WR improvement rate reflects a standard pattern of diminishing returns: as players approach the theoretical TAS limit, achieving further WR improvements becomes increasingly difficult. In other words, the remaining room for improvement steadily decreases.
    \end{itemize}
    \item The second dataset contains 1,353 attempts by the current WR holder, Niftski. Each attempt is split into eight major checkpoints, where 1 marks a successful completion and 0 marks a failure. Refer to Table 1 in the Appendix for an overview of the data.
    \begin{itemize}
        \item The table below shows Niftski's success rate at each major checkpoint. \\ \\
        \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
            \hline
            {Checkpoint} & {1-1} & {1-2} & {4-1} & {4-2} & {8-1} & {8-2} & {8-3} & {8-4} \\ \hline
            {Success} & {829} & {595} & {482} & {63} & {30} & {11} & {9} & {1} \\ \hline
            {$\mathbb{P}$(Success)} & {61.27\%} & {43.98\%} & {35.62\%} & {4.66\%} & {2.22\%} & {0.81\%} & {0.67\%} & {0.07\%} \\ \hline 
        \end{tabular} \\
        \item There are sharp decrease in the number of successful attempts at later checkpoints. For example, success falls from \textbf{482} at 4-1 to \textbf{63} at 4-2, an \textbf{87\%} decrease. There is also a \textbf{63\%} decrease between 8-1 and 8-2. A few factors can explain these drops:
        \begin{itemize}
            \item More pressure and stress in later stages, which can lead to mistakes.
            \item Higher difficulty in later stages, which increases the chance of errors.
            \item Success on a later level depends on success on earlier levels.
        \end{itemize}
    \end{itemize}

\end{itemize}

\vspace{0.8em}

\noindent \textbf{Suggestion of Statistical Model} \\
We suggest several models that fit the goal of the project and the data:
\begin{itemize}[leftmargin=1.2em]
    \item An ARIMA model that uses WR improvement over time (seconds per day) to predict when the WR will reach the TAS time.
    \item Bayesian model to handle the high levels of uncertainty that comes with small probabilities.
    \item Firth's bias reduction model using \textsf[logistf} to help with the smaller sample size due to the small chance of a world record happening.
    \item Other models as recommended and as necessary...
\end{itemize}


\vspace{0.8em}

\noindent \textbf{Limitations:}
\begin{itemize}[leftmargin=1.2em]
    \item Research on games like \emph{Super Mario Bros.} is limited, especially in academic settings. Because of this, data on the game and its time-attack attempts is limited.
    \item We do not have data from other competitive speedrunners and chose not to collect or publish any of their runs.
    \item We plan to address these limits by collecting data manually based on recorded results. These results are timed with reliable stopwatches to keep the measurements consistent.
\end{itemize}

\vspace{0.8em}

\noindent \textbf{Appendix:}
\begin{itemize}[leftmargin=1.2em]
    \item \textbf{R code:}
\begin{verbatim}
---
title: "Project"
author: "Theo and Thomas"
date: "October 2025"
output:
  pdf_document:
    toc: false
    number_sections: false
fontsize: 11pt
geometry: margin=1in
urlcolor: blue
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE,
                      fig.width = 6, fig.height = 4)
library(tidyverse)
library(knitr)
library(readxl)
library(MASS)
library(dplyr)
library(car)
library(broom)
library(GGally)
library(glmmTMB)
library(logistf)
library(ggplot2)
```


```{r}
#Read in data (data is ordered)
data = read.csv("Nifski Ordered.csv")
# Create composite features
data = data %>%
  mutate(
    # Overall success rate up to W8_3
    success_rate = (W1_1 + W1_2 + W4_1 + W4_2 + W8_1 + W8_2 + W8_3) / 7
  )
glimpse(data)
```



```{r}
# Build Poisson regression model:
pois_8_4 = glm(W8_4 ~ W1_1 + W1_2 + W4_1 + W4_2 + W8_1 + W8_2 + W8_3, data   = data, family = poisson)

# Look at basic model output (no interpretation here)
summary(pois_8_4)

# (Optional) compare to null model and basic GOF checks, following class notes
null_pois_8_4 = glm(W8_4 ~ 1, data = data, family = poisson)

# Likelihood ratio test vs null
anova(null_pois_8_4, pois_8_4, test = "Chisq")

#Poisson doesn't look very good, binomial is next
```

```{r}
# Fit logistic regression model
logit_8_4 = glm(W8_4 ~ W1_1 + W1_2 + W4_1 + W4_2 + W8_1 + W8_2 + W8_3, data   = data, family = binomial(link = "logit"))

summary(logit_8_4)

# Null (intercept-only) model
null_logit_8_4 = glm(W8_4 ~ 1, data = data, family = binomial(link = "logit"))

anova(null_logit_8_4, logit_8_4, test = "Chisq")

#binomial doesn't look good either
```

```{r}
#Negative binomial because its success into success into success that makes it to the last stage
nb_model = glm.nb(W8_4 ~ W1_1 + W1_2 + W4_1 + W4_2 + W8_1 + W8_2 + W8_3, data = data)

summary(nb_model)

```


```{r}
#logistic regression with ZIP is best bet, doing research now
zib_0 = glmmTMB(
  W8_4 ~ 1,        # count (binomial) part: intercept only
  ziformula = ~ 1, # zero-inflation part: intercept only
  family = binomial(link = logit),
  data = data
)

summary(zib_0)

zib_full = glmmTMB(
  W8_4 ~ W1_1 + W1_2 + W4_1 + W4_2 + W8_1 + W8_2 + W8_3,        # count (binomial) part: intercept only
  ziformula = ~ W1_1 + W1_2 + W4_1 + W4_2 + W8_1 + W8_2 + W8_3, # zero-inflation part: intercept only
  family = binomial(link = logit),
  data = data
)
summary(zib_full)
#no warning but still NAN p values, ZIP doesn't work
``` 

```{r}
firth_8_4 = logistf(W8_4 ~ W1_1 + W1_2 + W4_1 + W4_2 + W8_1 + W8_2 + W8_3, 
                     data = data)
summary(firth_8_4)

firth_simple = logistf(W8_4 ~ success_rate, data = data)
summary(firth_simple) #finally a significant p-val in just using success rate
```
Block for testing firth_simple
Actually a good fit, decent predictions, Nifski has around a 21% completion rate on average for all previous levels. dataset updated with predictions from this.

```{r}
firth_simple = logistf(W8_4 ~ success_rate, data = data)
summary(firth_simple)

# 1. Generate predictions for your existing data
data$predicted_prob = predict(firth_simple, type = "response")

# 2. Look at predictions for different success rates
# Create a sequence of success rates from 0 to 1
new_data = data.frame(success_rate = seq(0, 1, by = 0.1))
new_data$predicted_prob = predict(firth_simple, newdata = new_data, type = "response")

print("Predicted probabilities of breaking world record:")
print(new_data)

# 3. Summary statistics of predictions
cat("\nPrediction Summary:\n")
cat("Min predicted probability:", min(data$predicted_prob), "\n")
cat("Max predicted probability:", max(data$predicted_prob), "\n")
cat("Mean predicted probability:", mean(data$predicted_prob), "\n")
cat("Actual world record rate:", mean(data$W8_4), "\n")

# 4. DIAGNOSTIC PLOT 1: Predicted probabilities vs actual outcomes
ggplot(data, aes(x = predicted_prob, fill = as.factor(W8_4))) +
  geom_histogram(position = "identity", alpha = 0.6, bins = 30) +
  labs(title = "Distribution of Predicted Probabilities",
       x = "Predicted Probability of World Record",
       y = "Count",
       fill = "Actual WR") +
  scale_fill_manual(values = c("0" = "red", "1" = "green"),
                    labels = c("No WR", "WR")) +
  theme_minimal()



# 6. DIAGNOSTIC PLOT 3: Success rate vs probability curve
ggplot(data, aes(x = success_rate, y = predicted_prob)) +
  geom_point(aes(color = as.factor(W8_4)), alpha = 0.5) +
  geom_smooth(method = "loess", se = TRUE, color = "blue") +
  labs(title = "Success Rate vs Predicted Probability",
       x = "Success Rate (prior levels)",
       y = "Predicted Probability of World Record",
       color = "Actual WR") +
  scale_color_manual(values = c("0" = "red", "1" = "green"),
                     labels = c("No WR", "WR")) +
  theme_minimal()

# 7. ROC CURVE and AUC (measure of discrimination)
library(pROC)
roc_obj = roc(data$W8_4, data$predicted_prob)
auc_value = auc(roc_obj)

plot(roc_obj, main = paste("ROC Curve (AUC =", round(auc_value, 3), ")"))
cat("\nAUC (Area Under Curve):", auc_value, "\n")
cat("Interpretation: AUC > 0.7 = acceptable, > 0.8 = excellent\n")


# 9. Hosmer-Lemeshow test (goodness of fit)
library(ResourceSelection)
hl_test = hoslem.test(data$W8_4, data$predicted_prob, g = 10)
print(hl_test)
cat("Hosmer-Lemeshow test: p >0.05 indicates good fit\n")

# 10. Example predictions for specific scenarios
example_scenarios = data.frame(
  scenario = c("Beginner", "Intermediate", "Advanced", "Expert"),
  success_rate = c(0.2, 0.5, 0.8, 0.95)
)

example_scenarios$WR_probability = predict(firth_simple, 
                                            newdata = example_scenarios, 
                                            type = "response")

cat("\n Example Predictions \n")
print(example_scenarios)


target_row = data[
  data$W1_1 == 1 &
  data$W1_2 == 1 &
  data$W4_1 == 1 &
  data$W4_2 == 1 &
  data$W8_1 == 1 &
  data$W8_2 == 1 &
  data$W8_3 == 1,
]
target_row
```



Bayesian Start Here


```{r}
#Final model can output probability of WR on next attempt given current performance...
#This can be used to simulate the Probability of a WR within N attempts 1 - (1 - p_hat)^N
#bayesia  n model building
stages = c("W1_1", "W1_2", "W4_1", "W4_2", "W8_1", "W8_2", "W8_3", "W8_4")
stage_stats = tibble()

for (k in seq_along(stages)) {
  stage_name = stages[k]
  
  # "Reached" this stage = succeeded on all previous stages
  if (k == 1) {
    reached = rep(TRUE, nrow(data))
  } else {
    prev_stages = stages[1:(k-1)]
    reached = apply(data[, prev_stages, drop = FALSE] == 1, 1, all)
  }
  
  n_k = sum(reached)                              # number of attempts that reached this stage
  y_k = sum(data[[stage_name]][reached] == 1)     # successes at this stage
  
  stage_stats = bind_rows(
    stage_stats,
    tibble(
      stage     = stage_name,
      index     = k,
      reached_n = n_k,
      success_y = y_k,
      p_hat     = ifelse(n_k > 0, y_k / n_k, NA_real_)
    )
  )
}

stage_stats

```

```{r}
stage_stats %>%
  ggplot(aes(x = reorder(stage, index), y = p_hat)) +
  geom_col() +
  ylim(0, 1) +
  labs(
    title = "Empirical Conditional Success Probability by Stage",
    x = "Stage",
    y = "Estimated P(success | reached stage)"
  ) +
  theme_bw()
```

```{r}
alpha0 = 1
beta0  = 1

stage_stats = stage_stats %>%
  mutate(
    alpha_post = alpha0 + success_y,
    beta_post  = beta0 + reached_n - success_y,
    mean_post  = alpha_post / (alpha_post + beta_post),
    lower95    = qbeta(0.025, alpha_post, beta_post),
    upper95    = qbeta(0.975, alpha_post, beta_post)
  )

stage_stats

```

```{r}

stage_stats %>%
  ggplot(aes(x = reorder(stage, index), y = mean_post)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = lower95, ymax = upper95), width = 0.2) +
  ylim(0, 1) +
  labs(
    title = "Posterior Success Probability by Stage (Beta-Binomial)",
    x = "Stage",
    y = "Posterior mean P(success | reached stage)"
  ) +
  theme_bw()
```

```{r}
set.seed(123)

n_sims = 100000

# Draw samples from each stage's posterior
p_draws_list = lapply(seq_len(nrow(stage_stats)), function(i) {
  a = stage_stats$alpha_post[i]
  b = stage_stats$beta_post[i]
  rbeta(n_sims, a, b)
})

# Convert list to matrix: rows = sims, cols = stages
p_draws_mat = do.call(cbind, p_draws_list)

# Product across stages for each simulation = WR probability in that simulation
wr_prob_draws = apply(p_draws_mat, 1, prod)

# Summaries
wr_summary = tibble(
  mean       = mean(wr_prob_draws),
  median     = median(wr_prob_draws),
  lower95    = quantile(wr_prob_draws, 0.025),
  upper95    = quantile(wr_prob_draws, 0.975)
)

wr_summary
```

```{r}
tibble(wr_prob = wr_prob_draws) %>%
  ggplot(aes(x = wr_prob)) +
  geom_histogram(bins = 50) +
  labs(
    title = "Posterior Distribution of WR Probability (Per Attempt)",
    x = "P(WR on a single attempt)",
    y = "Frequency"
  ) +
  theme_bw()
```

```{r}
tibble(log10_wr_prob = log10(wr_prob_draws)) %>%
  ggplot(aes(x = log10_wr_prob)) +
  geom_histogram(bins = 50) +
  labs(
    title = "Posterior Distribution of log10 WR Probability",
    x = "log10 P(WR per attempt)",
    y = "Frequency"
  ) +
  theme_bw()
```



\end{verbatim}
    \item \textbf{Relevant visualizations:}

\begin{figure}[h]
  \centering
  \includegraphics[width=1\textwidth]{Final Project 1.png}
  \caption{World Record progression from 2002 to 2025.}
  \label{fig:boxplot}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=1\textwidth]{Final Project 2.png}
  \caption{World Record average improvement over time.}
  \label{fig:boxplot}
\end{figure}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|} \hline
Run/Stage & X1.1 & X1.2 & X4.1 & X4.2 & X8.1 & X8.2 & X8.3 & X8.4 \\ \hline 
1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\ \hline
2 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ \hline
3 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\ \hline
4 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 \\ \hline
5 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ \hline
6 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\ \hline
\end{tabular}
\caption{Overview of Niftski's runs.}
\end{table}


\end{itemize}

\end{document}


