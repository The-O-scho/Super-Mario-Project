---
title: "Project"
author: "Theo and Thomas"
date: "October 2025"
output:
  pdf_document:
    toc: false
    number_sections: false
fontsize: 11pt
geometry: margin=1in
urlcolor: blue
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE,
                      fig.width = 6, fig.height = 4)
library(tidyverse)
library(knitr)
library(readxl)
library(MASS)
library(dplyr)
library(car)
library(broom)
library(GGally)
library(glmmTMB)
library(logistf)
library(ggplot2)
library(ResourceSelection)
```

Create Data
```{r}
#Read in data (data is ordered)
data = read.csv("Nifski Ordered.csv")

# Create composite features
data = data %>%
  mutate(
    # Overall success rate up to W8_3
    success_rate = (W1_1 + W1_2 + W4_1 + W4_2 + W8_1 + W8_2 + W8_3) / 7
  )
glimpse(data)
```

Poisson
```{r}
# Build Poisson regression model:
pois_8_4 = glm(W8_4 ~ W1_1 + W1_2 + W4_1 + W4_2 + W8_1 + W8_2 + W8_3, data   = data, family = poisson)


summary(pois_8_4)

null_pois_8_4 = glm(W8_4 ~ 1, data = data, family = poisson)

anova(null_pois_8_4, pois_8_4, test = "Chisq")

#Poisson doesn't look very good, binomial is next
```

logistic binomial
```{r}
# Fit logistic regression model
logit_8_4 = glm(W8_4 ~ W1_1 + W1_2 + W4_1 + W4_2 + W8_1 + W8_2 + W8_3, data   = data, family = binomial(link = "logit"))

summary(logit_8_4)

null_logit_8_4 = glm(W8_4 ~ 1, data = data, family = binomial(link = "logit"))

anova(null_logit_8_4, logit_8_4, test = "Chisq")

#binomial doesn't look good either
```

Negative Binomial
```{r}
#Negative binomial because its success into success into success that makes it to the last stage
nb_model = glm.nb(W8_4 ~ W1_1 + W1_2 + W4_1 + W4_2 + W8_1 + W8_2 + W8_3, data = data)

summary(nb_model)

```

ZIP
```{r}
#logistic binomial regression with ZIP 
zib_0 = glmmTMB(
  W8_4 ~ 1,        
  ziformula = ~ 1,
  family = binomial(link = logit),
  data = data
)

summary(zib_0)

zib_full = glmmTMB(
  W8_4 ~ W1_1 + W1_2 + W4_1 + W4_2 + W8_1 + W8_2 + W8_3,        
  ziformula = ~ W1_1 + W1_2 + W4_1 + W4_2 + W8_1 + W8_2 + W8_3, 
  family = binomial(link = logit),
  data = data
)
summary(zib_full)
#no warning but still NAN p values, ZIP doesn't work

zib_full_2 = glmmTMB(
  W8_4 ~ success_rate,        
  ziformula = ~ success_rate, 
  family = binomial(link = logit),
  data = data
)

summary(zib_full_2) #Even success_rate doesn't work
``` 


Firth's bias reduction
```{r}
firth_8_4 = logistf(W8_4 ~ W1_1 + W1_2 + W4_1 + W4_2 + W8_1 + W8_2 + W8_3, 
                     data = data)
summary(firth_8_4)

firth_simple = logistf(W8_4 ~ success_rate, data = data)
summary(firth_simple) #finally a significant p-val in just using success rate
```
Block for testing firth_simple
Actually a good fit, decent predictions, Nifski has around a 21% completion rate on average for all previous levels. dataset updated with predictions from this.

```{r}
firth_simple = logistf(W8_4 ~ success_rate, data = data)
summary(firth_simple)

# 1. Generate predictions for existing data
data$predicted_prob = predict(firth_simple, type = "response")


# 2. DIAGNOSTIC PLOT 3: Success rate vs probability curve
ggplot(data, aes(x = success_rate, y = predicted_prob)) +
  geom_point(aes(color = as.factor(W8_4)), alpha = 0.5) +
  geom_smooth(method = "loess", se = TRUE, color = "blue") +
  labs(title = "Success Rate vs Predicted Probability",
       x = "Success Rate (prior levels)",
       y = "Predicted Probability of World Record",
       color = "Actual WR") +
  scale_color_manual(values = c("0" = "red", "1" = "green"),
                     labels = c("No WR", "WR")) +
  theme_minimal()

# 3.goodness of fit
hoslem.test(data$W8_4, data$predicted_prob, g = 10)
cat("Test: p >0.05 indicates good fit\n")


target_row = data[
  data$W1_1 == 1 &
  data$W1_2 == 1 &
  data$W4_1 == 1 &
  data$W4_2 == 1 &
  data$W8_1 == 1 &
  data$W8_2 == 1 &
  data$W8_3 == 1,
]
target_row
```



























































































































































































Bayesian Start Here


```{r}
#Final model can output probability of WR on next attempt given current performance...
#This can be used to simulate the Probability of a WR within N attempts 1 - (1 - p_hat)^N
#bayesian model building
stages = c("W1_1", "W1_2", "W4_1", "W4_2", "W8_1", "W8_2", "W8_3", "W8_4")
stage_stats = tibble()

for (k in seq_along(stages)) {
  stage_name = stages[k]
  
  # "Reached" this stage = succeeded on all previous stages
  if (k == 1) {
    reached = rep(TRUE, nrow(data))
  } else {
    prev_stages = stages[1:(k-1)]
    reached = apply(data[, prev_stages, drop = FALSE] == 1, 1, all)
  }
  
  n_k = sum(reached)                              # number of attempts that reached this stage
  y_k = sum(data[[stage_name]][reached] == 1)     # successes at this stage
  
  stage_stats = bind_rows(
    stage_stats,
    tibble(
      stage     = stage_name,
      index     = k,
      reached_n = n_k,
      success_y = y_k,
      p_hat     = ifelse(n_k > 0, y_k / n_k, NA_real_)
    )
  )
}

stage_stats

```

```{r}
stage_stats %>%
  ggplot(aes(x = reorder(stage, index), y = p_hat)) +
  geom_col() +
  ylim(0, 1) +
  labs(
    title = "Empirical Conditional Success Probability by Stage",
    x = "Stage",
    y = "Estimated P(success | reached stage)"
  ) +
  theme_bw()
```

```{r}
alpha0 = 1
beta0  = 1

stage_stats = stage_stats %>%
  mutate(
    alpha_post = alpha0 + success_y,
    beta_post  = beta0 + reached_n - success_y,
    mean_post  = alpha_post / (alpha_post + beta_post),
    lower95    = qbeta(0.025, alpha_post, beta_post),
    upper95    = qbeta(0.975, alpha_post, beta_post)
  )

stage_stats

```

```{r}

stage_stats %>%
  ggplot(aes(x = reorder(stage, index), y = mean_post)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = lower95, ymax = upper95), width = 0.2) +
  ylim(0, 1) +
  labs(
    title = "Posterior Success Probability by Stage (Beta-Binomial)",
    x = "Stage",
    y = "Posterior mean P(success | reached stage)"
  ) +
  theme_bw()
```

```{r}
set.seed(123)

n_sims = 100000

# Draw samples from each stage's posterior
p_draws_list = lapply(seq_len(nrow(stage_stats)), function(i) {
  a = stage_stats$alpha_post[i]
  b = stage_stats$beta_post[i]
  rbeta(n_sims, a, b)
})

# Convert list to matrix: rows = sims, cols = stages
p_draws_mat = do.call(cbind, p_draws_list)

# Product across stages for each simulation = WR probability in that simulation
wr_prob_draws = apply(p_draws_mat, 1, prod)

# Summaries
wr_summary = tibble(
  mean       = mean(wr_prob_draws),
  median     = median(wr_prob_draws),
  lower95    = quantile(wr_prob_draws, 0.025),
  upper95    = quantile(wr_prob_draws, 0.975)
)

wr_summary
```

```{r}
tibble(wr_prob = wr_prob_draws) %>%
  ggplot(aes(x = wr_prob)) +
  geom_histogram(bins = 50) +
  labs(
    title = "Posterior Distribution of WR Probability (Per Attempt)",
    x = "P(WR on a single attempt)",
    y = "Frequency"
  ) +
  theme_bw()
```

```{r}
tibble(log10_wr_prob = log10(wr_prob_draws)) %>%
  ggplot(aes(x = log10_wr_prob)) +
  geom_histogram(bins = 50) +
  labs(
    title = "Posterior Distribution of log10 WR Probability",
    x = "log10 P(WR per attempt)",
    y = "Frequency"
  ) +
  theme_bw()
```






