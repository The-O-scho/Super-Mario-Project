n = n(),
.groups = 'drop'
)
# 1. Generate predictions for existing world_data
world_data $predicted_prob = predict(firth_simple, type = "response")
# 1. Generate predictions for existing world_data
world_data$predicted_prob = predict(firth_simple, type = "response")
firth_simple = logistf(W8_4 ~ success_rate, world_data  = world_data )
world_data
firth_simple = logistf(W8_4 ~ success_rate, data  = world_data )
summary(firth_simple)
# 1. Generate predictions for existing world_data
world_data $predicted_prob = predict(firth_simple, type = "response")
# 2. DIAGNOSTIC PLOT 3: Success rate vs probability curve
ggplot(world_data , aes(x = success_rate, y = predicted_prob)) +
geom_point(aes(color = as.factor(W8_4)), alpha = 0.5) +
geom_smooth(method = "loess", se = TRUE, color = "blue") +
labs(title = "Success Rate vs Predicted Probability",
x = "Success Rate (prior levels)",
y = "Predicted Probability of World Record",
color = "Actual WR") +
scale_color_manual(values = c("0" = "red", "1" = "green"),
labels = c("No WR", "WR")) +
theme_minimal()
# 3.goodness of fit
hoslem.test(world_data $W8_4, world_data $predicted_prob, g = 10)
cat("Test: p >0.05 indicates good fit\n")
target_row = world_data [
world_data $W1_1 == 1 &
world_data $W1_2 == 1 &
world_data $W4_1 == 1 &
world_data $W4_2 == 1 &
world_data $W8_1 == 1 &
world_data $W8_2 == 1 &
world_data $W8_3 == 1,
]
target_row
world_data
interval_data = read.csv("FINAL DATA.csv")
interval_data
#Read in world_data  (world_data  is ordered)
world_data   = read.csv("FINAL WORLD DATA.csv")
# Create composite features
world_data  = world_data  %>%
mutate(
# Overall success rate up to W8_3
success_rate = (W1_1 + W1_2 + W4_1 + W4_2 + W8_1 + W8_2 + W8_3) / 7
)
world_data
interval_data = interval_data   %>%
mutate(DayNum = as.numeric(gsub('Day_', '', Day)))
world_data = world_data  %>%
mutate(DayNum = as.numeric(gsub('Day_', '', Day)))
# Calculate success rate by day and interval
daily_interval_success = interval_data   %>%
group_by(DayNum, Interval) %>%
summarise(
success_rate = mean(Success),
n_attempts = n(),
.groups = 'drop'
)
# Calculate overall success rate by day (across all intervals)
daily_overall_success = interval_data   %>%
group_by(DayNum) %>%
summarise(
overall_success_rate = mean(Success),
total_attempts = n(),
.groups = 'drop'
)
# Calculate how far Nifski get on average each day
daily_progression = interval_data   %>%
group_by(DayNum, Run) %>%
summarise(
max_interval_reached = max(Interval[Success == 1]),
.groups = 'drop'
) %>%
group_by(DayNum) %>%
summarise(
avg_max_interval = mean(max_interval_reached),
median_max_interval = median(max_interval_reached),
.groups = 'drop'
)
# Graph 1: Overall Success Rate Over Time (Enhanced with Legend)
ggplot(daily_overall_success, aes(x = DayNum, y = overall_success_rate)) +
geom_point(aes(color = "Daily Success Rate"), size = 4, alpha = 0.7) +
geom_smooth(aes(color = "Smoothed Trend"),
method = "loess", se = FALSE, linewidth = 1.5) +
geom_smooth(aes(color = "Linear Trend"),
method = "lm", se = TRUE, linewidth = 1.2, alpha = 0.2) +
scale_color_manual(name = "Legend",
values = c("Daily Success Rate" = "steelblue",
"Smoothed Trend" = "darkblue",
"Linear Trend" = "red")) +
scale_y_continuous(labels = scales::percent_format(accuracy = 1),
limits = c(0.50, 0.70)) +
scale_x_continuous(breaks = seq(0, 21, by = 3)) +
labs(title = "Speedrun Performance Improvement Over Time",
subtitle = "Overall success rate across all intervals by day",
x = "Day",
y = "Success Rate") +
theme_minimal(base_size = 13) +
theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
plot.subtitle = element_text(hjust = 0.5, size = 12, color = "gray30"),
panel.grid.minor = element_blank(),
axis.title = element_text(face = "bold"),
legend.position = "right")
#Now that we've shown that there is improvement
# Model improvement over time on interval completion
cloglog_time = glm(Success ~ DayNum + factor(Interval),
data = interval_data,
family = binomial(link = "cloglog"))
summary(cloglog_time)
cloglog_full = glm(Success ~ DayNum * factor(Interval),
data = interval_data,
family = binomial(link = "cloglog"))
anova(cloglog_time, cloglog_full, test = "Chisq")
#saturated model is better so do stepwise to find best AIC
# Null model (intercept only)
cloglog_null = glm(Success ~ 1,
data = interval_data,
family = binomial(link = "cloglog"))
# Forward stepwise selection
forward_model = step(cloglog_null,
scope = list(lower = cloglog_null, upper = cloglog_full),
direction = "forward",
trace = 0)
# Backward stepwise selection
backward_model = step(cloglog_full,
direction = "backward",
trace = 0)
# Both directions stepwise selection
both_model = step(cloglog_null,
scope = list(lower = cloglog_null, upper = cloglog_full),
direction = "both",
trace = 0)
best_model = forward_model
summary(best_model)
#Best model is Success ~ factor(Interval) + DayNum + factor(Inveral):DayNum
# Analyze best_model fit
cat("P-value:", deviance_gof, "\n")
cat("Interpretation: p >0.05 indicates good fit\n")
# 3. Pearson Chi-Square Test
pearson_resid = residuals(best_model, type = "pearson")
pearson_chisq = sum(pearson_resid^2)
pearson_pval = 1 - pchisq(pearson_chisq, best_model$df.residual)
cat("\nPearson Chi-Square Test:\n")
cat("Chi-Square:", pearson_chisq, "\n")
cat("P-value:", pearson_pval, "\n")
# 4. Hosmer-Lemeshow Test
library(ResourceSelection)
hl_test = hoslem.test(interval_data$Success, fitted(best_model), g = 10)
print(hl_test)
cat("Hosmer-Lemeshow: p >0.05 indicates good fit\n")
# 5. McFadden's Pseudo R-squared
null_dev = best_model$null.deviance
resid_dev = best_model$deviance
pseudo_r2 = 1 - (resid_dev / null_dev)
cat("\nMcFadden's Pseudo R-squared:", pseudo_r2, "\n")
cat("Interpretation: 0.2-0.4 = excellent fit\n")
# 6. Calculate VIF for multicollinearity (if applicable)
cat("\nVariance Inflation Factors:\n")
vif_values = vif(best_model)
print(vif_values)
cat("VIF < 10 indicates no serious multicollinearity\n")
# 7. GRAPHS
# Graph 3: Residuals by Interval
ggplot(interval_data, aes(x = factor(Interval), y = deviance_resid)) +
geom_boxplot(fill = "lightblue", alpha = 0.7) +
geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
labs(title = "Residuals by Interval",
subtitle = "Checking if residuals vary by interval level",
x = "Interval",
y = "Deviance Residuals") +
theme_minimal() +
theme(plot.title = element_text(hjust = 0.5, face = "bold"))
ggplot(interval_data, aes(x = factor(Interval), y = deviance_resid)) +
geom_boxplot(fill = "lightblue", alpha = 0.7) +
geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
labs(title = "Residuals by Interval",
subtitle = "Checking if residuals vary by interval level",
x = "Interval",
y = "Deviance Residuals") +
theme_minimal() +
theme(plot.title = element_text(hjust = 0.5, face = "bold"))
# Get fitted probabilities for each observation from best_model
interval_data$stage_fitted_prob = fitted(best_model)
# Aggregate to run level - average predicted success across all stages
run_level_predictions = interval_data %>%
group_by(Run, Day, DayNum) %>%
summarise(
avg_predicted_success = mean(stage_fitted_prob),
min_predicted_success = min(stage_fitted_prob),  # weakest stage
.groups = 'drop'
)
# Merge with world_data
world_data = world_data %>%
left_join(run_level_predictions, by = c("Day", "DayNum"))
# Now use in your WR model
cloglog_wr_enhanced = glm(W8_4 ~ success_rate + avg_predicted_success + DayNum,
data = world_data,
family = binomial(link = "cloglog"))
summary(cloglog_wr_enhanced)
# Try the simplest model
cloglog_simple = glm(W8_4 ~ DayNum,
data = world_data,
family = binomial(link = "cloglog"))
# Try the simplest model
cloglog_simple = glm(W8_4 ~ DayNum,
data = world_data,
family = binomial(link = "cloglog"))
summary(cloglog_simple)
# Try with just success_rate and DayNum (no avg_predicted_success)
cloglog_two = glm(W8_4 ~ success_rate + DayNum,
data = world_data,
family = binomial(link = "cloglog"))
summary(cloglog_wr_enhanced)
summary(cloglog_simple)
summary(cloglog_two)
summary(cloglog_two)
# Test if improvement rate depends on skill level
cloglog_interaction = glm(W8_4 ~ success_rate * DayNum,
data = world_data,
family = binomial(link = "cloglog"))
AIC(cloglog_interaction)  # Compare to 1022.5
summary(cloglog_interaction)
# Get coefficients for DayNum interaction with each interval
coef_summary = summary(best_model)$coefficients
# Extract interaction terms
interaction_coefs = coef_summary[grepl("DayNum:factor", rownames(coef_summary)), ]
# Create interval-specific improvement rates
interval_improvement = data.frame(
Interval = 2:8,
improvement_rate = interaction_coefs[, "Estimate"]
)
# Get coefficients
coef_summary = summary(best_model)$coefficients
# Check what the interaction terms are actually called
print(rownames(coef_summary))
# Extract interaction terms - try different pattern
interaction_coefs = coef_summary[grepl("factor.*:DayNum|DayNum:factor", rownames(coef_summary)), ]
# If still empty, check the exact names
if(nrow(interaction_coefs) == 0) {
# Print all coefficient names to see the pattern
print("Coefficient names:")
print(rownames(coef_summary))
# Try this pattern instead
interaction_coefs = coef_summary[grepl(":DayNum", rownames(coef_summary)), ]
}
print("Interaction coefficients:")
print(interaction_coefs)
# Now create the data frame (adjust length as needed)
if(nrow(interaction_coefs) > 0) {
interval_improvement = data.frame(
Interval = 2:(1 + nrow(interaction_coefs)),
improvement_rate = interaction_coefs[, "Estimate"]
)
print(interval_improvement)
# Calculate weighted improvement score
world_data = world_data %>%
mutate(
weighted_improvement =
W1_1 * 0 +  # baseline (Interval 1 has no interaction term)
W1_2 * interval_improvement$improvement_rate[1] +
W4_1 * interval_improvement$improvement_rate[2] +
W4_2 * interval_improvement$improvement_rate[3] +
W8_1 * interval_improvement$improvement_rate[4] +
W8_2 * interval_improvement$improvement_rate[5] +
W8_3 * interval_improvement$improvement_rate[6]
)
# Use in model
firth_improved = logistf(W8_4 ~ success_rate + weighted_improvement + DayNum,
data = world_data)
summary(firth_improved)
} else {
cat("No interaction terms found. Check your best_model formula.\n")
cat("Your model should be: Success ~ DayNum * factor(Interval)\n")
}
summary(firth_improved)
firth_weighted_only = logistf(W8_4 ~ weighted_improvement,
data = world_data)
summary(firth_weighted_only)
# Best World Record Prediction Model
best_wr_model = glm(W8_4 ~ success_rate + DayNum,
data = world_data,
family = binomial(link = "cloglog"))
summary(best_wr_model)
# Check AIC
AIC(best_wr_model)
# Make predictions
world_data$wr_predicted_prob = predict(best_wr_model, type = "response")
# Example: Predict for perfect run on Day 22
perfect_run = data.frame(
success_rate = 1.0,
DayNum = 22
)
predict(best_wr_model, newdata = perfect_run, type = "response")
# Best World Record Prediction Model
best_wr_model = glm(W8_4 ~ success_rate + DayNum,
data = world_data,
family = binomial(link = "cloglog"))
summary(best_wr_model)
# Check AIC
AIC(best_wr_model)
# Make predictions
world_data$wr_predicted_prob = predict(best_wr_model, type = "response")
# Example: Predict for perfect run on Day 22
perfect_run = data.frame(
success_rate = 1.0,
DayNum = 22
)
predict(best_wr_model, newdata = perfect_run, type = "response")
world_data
best_wr_model = glm(W8_4 ~ success_rate + DayNum,
ziformula = ~ success_rate + DayNum,
data = world_data,
family = binomial(link = "cloglog"))
best_wr_model = glm(W8_4 ~ success_rate + DayNum,
ziformula = ~ success_rate + DayNum,
data = world_data,
family = binomial(link = "logit"))
best_wr_model = glmmTMB(W8_4 ~ success_rate + DayNum,
ziformula = ~ success_rate + DayNum,
data = world_data,
family = binomial(link = "cloglog"))
summary(best_wr_model)
best_wr_model = glmmTMB(W8_4 ~ success_rate + DayNum,
ziformula = ~ DayNum,
data = world_data,
family = binomial(link = "cloglog"))
summary(best_wr_model)
best_wr_model = glm(W8_4 ~ success_rate + DayNum,
data = world_data,
family = binomial(link = "cloglog"))
summary(best_wr_model)
best_wr_model2 = glm(W8_4 ~  DayNum,
data = world_data,
family = binomial(link = "cloglog"))
summary(best_wr_model2)
anova(best_wr_model2, best_wr_model)
AIC(best_wr_model)
AIC(best_wr_model2)
coef(best_wr_model)
logLik(best_wr_model2)
logLik(best_wr_model)
best_wr_model2 = glm(W8_4 ~  success_rate,
data = world_data,
family = binomial(link = "cloglog"))
summary(best_wr_model2)
AIC(best_wr_model)
best_wr_model = glm(W8_4 ~ success_rate + DayNum,
data = world_data,
family = binomial(link = "logit"))
best_wr_model2 = glm(W8_4 ~  DayNum,
data = world_data,
family = binomial(link = "logit"))
summary(best_wr_model)
summary(best_wr_model2)
world_data$success_rate
world_data$success_rate %>% summarize(.)
world_data$success_rate %>% summary(.)
hist(world_data$success_rate)
best_wr_model2 = glm(W8_4 ~  success_rate,
data = world_data,
family = binomial(link = "logit"))
summary(best_wr_model2)
test<- model.matrix(lm(~success_rate,data=world_data))
world_data %>% group_by(W8_4) %>% summary(avg=mean(success_rate))
world_data %>% group_by(W8_4) %>% summarize(avg=mean(success_rate))
successes<-world_data %>% filter(W8_4==1)
hist(successes$success_rate)
not_success<-world_data %>% filter(W8_4!=1)
max(not_success$success_rate)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE,
fig.width = 6, fig.height = 4)
library(tidyverse)
library(knitr)
library(readxl)
library(MASS)
library(dplyr)
library(car)
library(broom)
library(GGally)
library(glmmTMB)
library(logistf)
library(ggplot2)
library(ResourceSelection)
interval_data
interval_data = read.csv("FINAL DATA.csv")
interval_data
interval_data = read.csv("FINAL DATA.csv")
interval_data
#Read in world_data  (world_data  is ordered)
world_data   = read.csv("FINAL WORLD DATA.csv")
world_data
interval_data = interval_data   %>%
mutate(DayNum = as.numeric(gsub('Day_', '', Day)))
world_data = world_data  %>%
mutate(DayNum = as.numeric(gsub('Day_', '', Day)))
# Calculate success rate by day and interval
daily_interval_success = interval_data   %>%
group_by(DayNum, Interval) %>%
summarise(
success_rate = mean(Success),
n_attempts = n(),
.groups = 'drop'
)
# Calculate overall success rate by day (across all intervals)
daily_overall_success = interval_data   %>%
group_by(DayNum) %>%
summarise(
overall_success_rate = mean(Success),
total_attempts = n(),
.groups = 'drop'
)
# Calculate how far Nifski get on average each day
daily_progression = interval_data   %>%
group_by(DayNum, Run) %>%
summarise(
max_interval_reached = max(Interval[Success == 1]),
.groups = 'drop'
) %>%
group_by(DayNum) %>%
summarise(
avg_max_interval = mean(max_interval_reached),
median_max_interval = median(max_interval_reached),
.groups = 'drop'
)
interval_data
interval_models
# Build separate models for each interval
interval_models = list()
intervals_to_model = c("1-1", "1-2", "4-1", "4-2", "8-1", "8-2", "8-3")
interval_models
intervals_to_model
FINAL DATA
interval_data
# Initialize empty list
interval_models = list()
# Map intervals to their corresponding world record columns
interval_mapping = c(
"1" = "W1_1",
"2" = "W1_2",
"3" = "W4_1",
"4" = "W4_2",
"5" = "W8_1",
"6" = "W8_2",
"7" = "W8_3",
"8" = "W8_4"
)
# Build models for intervals 1-7 (we're predicting interval 8/W8_4)
for (interval_num in 1:7) {
cat("\n=== Processing Interval", interval_num, "(", interval_mapping[as.character(interval_num)], ") ===\n")
# Filter data for this specific interval
interval_subset = interval_data %>%
filter(Interval == interval_num)
cat("Number of observations:", nrow(interval_subset), "\n")
cat("Success rate:", mean(interval_subset$Success), "\n")
# Build cloglog model for this interval
model = glm(Success ~ DayNum,
data = interval_subset,
family = binomial(link = "cloglog"))
# Store the model with the world column name as key
world_col = interval_mapping[as.character(interval_num)]
interval_models[[world_col]] = model
cat("Model successfully fit!\n")
print(summary(model))
}
# Now create predictions for each run in world_data
world_data = world_data %>%
mutate(
pred_W1_1 = predict(interval_models[["W1_1"]],
newdata = data.frame(DayNum = DayNum),
type = "response"),
pred_W1_2 = predict(interval_models[["W1_2"]],
newdata = data.frame(DayNum = DayNum),
type = "response"),
pred_W4_1 = predict(interval_models[["W4_1"]],
newdata = data.frame(DayNum = DayNum),
type = "response"),
pred_W4_2 = predict(interval_models[["W4_2"]],
newdata = data.frame(DayNum = DayNum),
type = "response"),
pred_W8_1 = predict(interval_models[["W8_1"]],
newdata = data.frame(DayNum = DayNum),
type = "response"),
pred_W8_2 = predict(interval_models[["W8_2"]],
newdata = data.frame(DayNum = DayNum),
type = "response"),
pred_W8_3 = predict(interval_models[["W8_3"]],
newdata = data.frame(DayNum = DayNum),
type = "response")
)
# Calculate overall WR probability as product of all interval probabilities
world_data = world_data %>%
mutate(
pred_WR_prob = pred_W1_1 * pred_W1_2 * pred_W4_1 * pred_W4_2 *
pred_W8_1 * pred_W8_2 * pred_W8_3
)
# Now use this predicted WR probability in your final model
final_model = logistf(W8_4 ~ pred_WR_prob, data = world_data)
summary(final_model)
# Check the predictions
world_data %>%
select(DayNum, starts_with("pred_"), W8_4) %>%
head(20)
# Check the predictions
world_data %>%
dplyr::select(DayNum, starts_with("pred_"), W8_4) %>%
head(20)
# Check the predictions
world_data %>%
dplyr::select(DayNum, starts_with("pred_"), W8_4)
